\chapter{Nash Equilibria}
\label{chapter:Nash}
%\documentclass[10pt]{article}
%\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%			Introduction Chapter 1				%%%%%%
%%%%%												%%%%%%
%%%%%												%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Nash Equilibria}
-- rechtstreeks uit FlipIt paper --\\
As a second step, we are interested in finding Nash equilibria, points
for which neither player will increase his benefit by changing his rate of play. More
formally, a Nash equilibrium for the periodic game is a point $(\alpha_{0}^{*},\alpha_{1}^{*})$ such that
the defender's benefit $\beta_{0}(\alpha_{0},\alpha_{1}^{*}) $is maximized at $\alpha_{0}= \alpha_{0}^{*}$ and the attacker's benefit
$\beta_{1}(\alpha_{0}^{*},\alpha_{1}) $ is maximized at $\alpha_{1}= \alpha_{1}^{*}$ .
To begin with, some useful notation. We denote by opt0($\alpha_{1}$) the set of values (rates
of play $\alpha_{0}$) that optimize the benefit of the defender for a fixed rate of play $\alpha_{1}$ of the
attacker. Similarly, we denote by opt1($\alpha_{0}$) the set of values (rates of play $\alpha_{1}$) that optimize
the benefit of the attacker for a fixed rate of play $\alpha_{0}$ of the defender. The following
theorem specifies Nash equilibria for the periodic game and is proven in Appendix A. \\

Nash equilibria are points whith the property that neither player benefits by deviating in isolaition form equilibrium. We can compute Nash Equilibria for the periodic game as an intersection points of curvest opt0 and opt1. To determine opt0(a0) we need to compute the derivate of  $\beta_{0}(\alpha_{0},\alpha_{1}) $ for a fixed $\alpha_{1}$. We consider two cases:\\
-----

\textbf{Case: $\delta_{D} \leq \delta_{A} $}\\

\begin{equation}
\beta_{D}(\alpha_{D},\alpha_{A}) = 1 - \dfrac{\delta_{D}}{2\delta_{A}} - \dfrac{k_{D}}{\delta_{D}} - \dfrac{d^{2}}{2\delta_{D}\delta_{A}} + \dfrac{d}{\delta_{A}}
\end{equation}
If we take the partial derivative we get the following result:
\begin{equation}\label{formdelta}
\frac{\partial \beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} = - \dfrac{1}{2\delta_{A}} + \dfrac{k_{D}}{\delta_{D}^{2}} + \dfrac{d^{2}}{2\delta_{D}^{2}\delta_{A}}
\end{equation}

Als we de formule \ref{formdelta} gelijkstellen aan 0 krijgen we:

\begin{equation}
\frac{\partial \beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} =0 ~~~~~~ =>~~~~~~ \delta_{D} = \sqrt{2\delta_{A}k_{D} + d^{2}}
\end{equation}

The function increases on $[0, \sqrt{2\delta_{A}k_{D} + d^{2}}]$ and is decreasing on $[\sqrt{2\delta_{A}k_{D} + d^{2}}, \infty]$ So we got a maximum on $\delta_{D} = minimum \{ \delta_{A}, \sqrt{2\delta_{A}k_{D} + d^{2}} \} $ \\
~~\\


\textbf{Case 2.A: $\delta_{D} \geq d+\delta_{A} \geq \delta_{A} $ }\\


 
\begin{equation}
\dfrac{t \beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} = \dfrac{\delta_{A}}{2\delta_{D}} + \dfrac{d}{\delta_{D}} - \dfrac{k_{D}}{\delta_{D}}
\end{equation}
\begin{equation}
\frac{\partial \beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} = -\dfrac{\delta_{A}}{2\delta_{D}^{2}} - \dfrac{d}{\delta_{D}^{2}} + \dfrac{k_{D}}{\delta_{D}^{2}}
\end{equation}

Als we de formule \ref{formdelta} gelijkstellen aan 0 krijgen we:
\begin{equation}
\frac{\partial \beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} =0 ~~~~~~ =>~~~~~~ \delta_{A} = 2(k_{D}-d) = dk_{D} - 2d
\end{equation}
\begin{description}
\item If $k_{D} < d$ 
\begin{description}
\item decreasing $ \delta_{A} < 2(k_{D} -d)$
\item increasing  $\delta_{A} > 2(k_{D} -d)$ \todo{equal}
\end{description}
\item If $k_{D} > d$ 
\begin{description}
\item increasing $ \delta_{A} < 2(k_{D} -d)$
\item decreasing  $\delta_{A} > 2(k_{D} -d)$ \todo{equal}
\end{description}
\end{description}
~~\\

\textbf{Case 2.B: $d+\delta_{A} \geq \delta_{D} \geq  \delta_{A} $} \\

\begin{equation}
\dfrac{\beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} = \dfrac{\delta_{A}}{2\delta_{D}} + \dfrac{d}{\delta_{D}} - \dfrac{k_{D}}{\delta_{D}} - \dfrac{(d-(\delta_{D} - \delta_{A}))^{2}}{d\delta_{D}\delta_{A}}
\end{equation}


\begin{equation}
\frac{\partial \beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} =  - \dfrac{1}{2\delta_{D}} + \dfrac{k_{D}}{\delta_{D}^{2}} + \dfrac{d^{2}}{2\delta_{D}^{2}\delta_{A}}
\end{equation}

Als we de formule \ref{formdelta} gelijkstellen aan 0 krijgen we:

\begin{equation}
\frac{\partial \beta_{D}(\alpha_{D},\alpha_{A})}{\partial \alpha_{D}} =0 ~~~~~~ =>~~~~~~ \delta_{D} = \sqrt{2\delta_{A}k_{D} + d^{2}}
\end{equation}


The function increases on $[0, \sqrt{2\delta_{A}k_{D} + d^{2}}]$ and is decreasing on $[\sqrt{2\delta_{A}k_{D} + d^{2}}, \infty]$ So we got a maximum on $\delta_{D} = minimum \{ \delta_{A}, \sqrt{2\delta_{A}k_{D} + d^{2}} \} $ \\


The optimum functions will be piecewise functions. There will be different optimum functions depending on $k_{D}$ and $d$. Two cases are considered.
\subsubsection{$k_{D} \leq d$}

\begin{description}
\item if $\delta_{A} < 2(k_{D} - d)$ \\
For case 2.A it is decreasing. Because $k_{D} \leq d$ the term $2(k_{D} - d)$ will always be negative so $\delta_{A}$ will also be negative. This means for case 1 and case 2.b that $\delta_{D} = 0$ and so the defender will not play.
\item $\delta_{A} = 2(k_{D} - d)$ \\
$\delta_{A} = 0$ so the attacker will not play. For case 1 and case 2.b the defender will also not play.
\item $\delta_{A} > 2(k_{D} - d)$ \\
For case 2.b it is decreasing. Case 2.a it is increasing. For case 1 $2(k_{D} - d)$ is always negative so the minimum is 0. This means that $\delta_{D} \in [0, \sqrt{d^{2} + 2\delta_{A}k_{D}}]$
\end{description}

\subsection{Best responses}
\subsubsection{$k_{D} > d$}
\begin{description}
\item if $\delta_{A} < 2(k_{D} - d)$ \\
For case 2.A it is increasing. 
\item $\delta_{A} = 2(k_{D} - d)$ \\
\item $\delta_{A} > 2(k_{D} - d)$ \\
In every case the function is decreasing so the defender will not play.
\end{description}

From this analyses we can compute $opt_{D}(\delta_{A})$ for two different cases as:
 \begin{displaymath}
  opt_{D}(\delta_{A}) = \left\{
     \begin{array}{lr}
       0, & \delta_{A} < 2(k_{D} - d)\\
       0, & \delta_{A} = 2(k_{D} - d) \\
       \big[0,\sqrt{d^{2} + 2\delta_{A}k_{D}}\big], & \delta_{A} > 2(k_{D} - d)
     \end{array}
   \right.
\end{displaymath}

For case :
 \begin{displaymath}
  opt_{D}(\delta_{A}) = \left\{
     \begin{array}{lr}
       0, & \delta_{A} < 2(k_{D} - d)\\
       0, & \delta_{A} = 2(k_{D} - d) \\
       \big[0,\sqrt{d^{2} + 2\delta_{A}k_{D}}\big], & \delta_{A} > 2(k_{D} - d)
     \end{array}
   \right.
\end{displaymath}
-------------------------------------------\\

\subsection{betaA}
We still consider the case where $d < \delta_{D}$. \\
To determine the Nash equilibria we also need to determine $opt_{1}(\delta_{D})$ by computing the derivative of $\beta_{A}(\delta_{D},\delta_{A})$ for a fixed $\delta_{D}$. We consider 2 cases: \\

\textbf{Case 1: $\delta_{A} \geq \delta_{D}$}\\

Since 
\begin{equation*}
\beta_{A}(\delta_{D},\delta_{A}) =\dfrac{\delta_{D}}{2\delta_{A}} - \dfrac{k_{A}}{\delta_{A}} + \dfrac{d^{2}}{2\delta_{D}\delta_{A}^{2}} - \dfrac{d}{\delta_{A}}
\end{equation*}
the derivative is:
\begin{equation*}
\dfrac{\partial \beta_{A}(\delta_{D},\delta_{A})}{\partial \delta_{A}} = -\dfrac{\delta_{D}}{2\delta_{A}^{2}} + \dfrac{k_{A}}{\delta_{A}^{2}} - \dfrac{d^{2}}{2\delta_{D}\delta_{A}^{2}} + \dfrac{d}{\delta_{A}^{2}}
\end{equation*}
it follows that $\beta_{A}(\delta_{D},\cdot)$ is increasing if $2k_{A} < (\delta_{D} - d)^{2} / \delta_{D}$ and decreasing if $2k_{A} > (\delta_{D} - d)^{2} / \delta_{D}$. \\

\textbf{Case 2: $\delta_{A} \leq \delta_{D}$ } \\

\textbf{A}: $\delta_{A} \leq d + \delta_{A} \leq \delta_{D}$ \\
Since 
\begin{equation*}
\beta_{A}(\delta_{D},\delta_{A}) =1- \dfrac{\delta_{A}}{2\delta_{D}} - \dfrac{k_{A}}{\delta_{A}} - \dfrac{d}{\delta_{D}}
\end{equation*}
the derivative is:
\begin{equation*}
\dfrac{\partial \beta_{A}(\delta_{D},\delta_{A})}{\partial \delta_{A}} = \dfrac{-1}{2\delta_{D}} + \dfrac{k_{A}}{\delta_{A}^{2}}
\end{equation*}
it follows that $\beta_{A}(\delta_{D},\cdot)$ is increasing on $[0,\sqrt{2k_{A}\delta_{D}}]$ and decreasing on $[\sqrt{2k_{A}\delta_{D}}, \infty]$ and thus has a maximum on $\delta_{A} = maximum \{\delta_{D}, \sqrt{2k_{A}\delta_{D}} \} $. The maximum between $\delta_{D}$ and $ \sqrt{2k_{A}\delta_{D}}$ is needed because $\delta_{A} $ cannot exceed $\delta_{D}$ in this case. \\

\textbf{B}: $\delta_{A}  \leq \delta_{D} \leq d + \delta_{A} $ \\

Since 
\begin{equation*}
\beta_{A}(\delta_{D},\delta_{A}) = 1 - \dfrac{\delta_{A}}{2\delta_{D}} - \dfrac{d}{\delta_{A}} - \dfrac{k_{A}}{\delta_{A}} + \dfrac{(d-(\delta_{D}-\delta_{A})^{2}}{2\delta_{D}\delta_{A}} 
\end{equation*}
the derivative is:
\begin{equation*}
\dfrac{\partial \beta_{A}(\delta_{D},\delta_{A})}{\partial \delta_{A}} = -\dfrac{\delta_{D}}{2\delta_{A}^{2}} + \dfrac{k_{A}}{\delta_{A}^{2}} - \dfrac{d^{2}}{2\delta_{D}\delta_{A}^{2}} + \dfrac{d}{\delta_{A}^{2}}
\end{equation*}
it follows that $\beta_{A}(\delta_{D},\cdot)$ is increasing if $2k_{A} < (\delta_{D} - d)^{2} / \delta_{D}$ and decreasing if $2k_{A} > (\delta_{D} - d)^{2} / \delta_{D}$. This is the same result as in case 1.\\
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 

%\end{document}
